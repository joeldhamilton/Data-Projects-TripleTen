# Joel Hamilton

## 👨‍💻 Machine Learning Engineer | Data Scientist

Hello! I'm Joel, a recent TripleTen Data Science Bootcamp graduate with a passion for transforming complex data into actionable insights.

### 🚀 Quick Facts
- 🎓 TripleTen Data Science Bootcamp Graduate
- 🏛 BA in History from University of Wisconsin
- 💼 Background in Sales and Marketing
- 🌍 Based in Winston Salem, NC (Open to Remote Opportunities)

### 🛠 Skills
- **Languages:** Python, SQL
- **Machine Learning:** Supervised and Unsupervised Learning, NLP, Time Series Analysis
- **Libraries/Frameworks:** Pandas, Scikit-learn, TensorFlow, Keras
- **Tools:** Jupyter Notebook, Git
- **Soft Skills:** Analytical Thinking, Problem-Solving, Communication

### 🔍 Featured Projects
#### Telecom Customer Churn Prediction and Retention Strategy Optimization | July 2024 
- Developed a machine learning model to predict customer churn for Interconnect, a telecom operator, to enable proactive retention strategies and improve customer loyalty.
- Increased churn prediction accuracy by 84.66% compared to the baseline model, achieving an AUC-ROC score of 0.9233. Engineered 12 key features from raw customer data, performed comprehensive exploratory data analysis, and implemented advanced machine learning techniques including XGBoost and SHAP analysis.
- Identified top 3 predictors of customer churn (total charges, charge per month, and monthly charges), providing actionable insights for targeted retention efforts. Developed data-driven recommendations for optimizing pricing strategies, promoting longer-term contracts, and enhancing high-value services like fiber optic internet.
#### Automated Sentiment Analysis System for Movie Review Classification | July 2024
- The Film Junky Union, an emerging community for classic movie enthusiasts, required an automated system to classify movie reviews as positive or negative. The project aimed to develop a machine learning model to accurately detect sentiment in user-submitted reviews.
- Developed a high-performing sentiment analysis model that exceeded the project's target F1 score of 0.85, achieving an F1 score of 0.88 on the test set. This was accomplished by implementing and comparing multiple text classification models, including TF-IDF vectorization with Logistic Regression, LightGBM, and BERT embeddings.
- Optimized model selection and feature engineering processes, resulting in a computationally efficient solution that outperformed more complex models. This was achieved by conducting thorough exploratory data analysis, implementing various text preprocessing techniques, and performing comparative analysis of model performance across different architectures.
